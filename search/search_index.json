{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Athena AI","text":"<p>VISUAL INTRODUCTION HERE (30s)</p> <ul> <li>the problem we address</li> <li>how we solve it</li> <li>how is it different</li> </ul> <p>ENTERPRISE AI IN ACTION Discover how we help enterprises leverage Hybrid AI to build smarter interactive applications (3 min video): Note that this is not the right video, just an example of how to embed a video in this mockup. </p> <p></p> <p>To know more about Athena Decision Systems visit athenadecisions.com.</p>"},{"location":"Contribute/","title":"Contribute","text":""},{"location":"Contribute/#another-heading","title":"Another heading","text":"<p>Some text here</p>"},{"location":"Develop/1-Anatomy%20of%20a%20solution/","title":"Anatomy of a solution","text":"<p>This chapter presents the high level concepts and constructs of the Owl Agent framework and how it is used in the context of a custom solution.</p> <p>Agent is a deployable unit built up by choreographing one or more llm, each with its own workflow that can leverage external tools, and guided by prompts.</p> <p>A typical solution includes: 1/ a front end to let a human interact with the system, 2/ a backend to manage the life cycle of agents integrated with a LLM running as a service, with dynamic integration to tools and functions, vector store retrievers, and decision services. 3/ Decision service with inference rules to control the decision to be made.</p> <p></p> <p>4/ The conversation is stateful, persisted and integrated with the different agents defined in the solution. </p> <p>The Owl Agent backend may manage multiple concurrent conversations and multiple agents instances. It can scale horizontally too. </p>"},{"location":"Develop/1-Anatomy%20of%20a%20solution/#key-concepts","title":"Key concepts","text":"<p>The main concepts the framework defines and uses are:</p> <ul> <li>An Agent manages a co-ordinated set of calls to a Large Language Model,  with a prompt and tools, to accomplish a subtask.</li> <li>Prompts, are <code>System prompts</code> in the Generative AI context. They define what the LLM should do</li> </ul> <p>The following diagram presents, one agent that integrate one LLM, with a set of tools. One tool is helping to access a client given its name, queries a database, one tool is to compute the next best action is send structured data to a decision service, to get better decsion, and finally one is a retriever to access collections in a vector database to support Retrieved Augmented Generation use cases.</p> <p></p> <p>Tools can  be any python function, ot proxies to remote business service.</p>"},{"location":"Develop/1-Anatomy%20of%20a%20solution/#agents","title":"Agents","text":"<p>An agent is an interactive application or solution that supports a specific business use case, like helping a worker performing a specific task of a business process.  The execution of the agent involves the coordination of one or more LLMs.  Agents may be stateful to preserve the state of a conversation using snapshot capabilities. </p> <p>Agent management has two parts: 1/ the management of the OwlAgent entity definitions with REST resources and a persistence repository, and 2/ the agent runner instance which manages conversations:</p> <p></p> <p>In any solution the conversation manager service use one agent runner.</p>"},{"location":"Develop/1-Anatomy%20of%20a%20solution/#promps","title":"Promps","text":""},{"location":"Develop/1-Anatomy%20of%20a%20solution/#documents","title":"Documents","text":""},{"location":"Develop/1-Anatomy%20of%20a%20solution/#tools","title":"Tools","text":""},{"location":"Develop/1-Anatomy%20of%20a%20solution/#agentic-workflow","title":"Agentic workflow","text":""},{"location":"Develop/1-Anatomy%20of%20a%20solution/#a-customizable-agent-backend","title":"A customizable agent backend","text":""},{"location":"Develop/1-Anatomy%20of%20a%20solution/#backend-apis","title":"Backend APIs","text":""},{"location":"Develop/1-Anatomy%20of%20a%20solution/#hooks-to-custom-code","title":"Hooks to custom code","text":"<p>Some text here</p>"},{"location":"Develop/1-Anatomy%20of%20a%20solution/#an-out-of-the-box-chatbot-frontend-webapp","title":"An out-of-the-box chatbot frontend webapp","text":""},{"location":"Develop/1-Anatomy%20of%20a%20solution/#how-the-frontend-and-backend-work-together","title":"How the frontend and backend work together","text":""},{"location":"Develop/2-Developers%20roadmap/","title":"Developer's roadmap to build a custom solution","text":"<p>This chapter addresses how to develop a custom solution and how to maintain or add new features to the current backend.</p>"},{"location":"Develop/2-Developers%20roadmap/#understand-which-approach-works-well-for-a-given-use-case","title":"Understand which approach works well for a given use case","text":"<p>LLMs are very good at providing linguistic capabilities such as:</p> <ul> <li>extracting data from text  </li> <li>translation  </li> <li>intent detection  </li> <li>sentiment analysis  </li> <li>summarization  </li> </ul> <p>On the other hand, LLMs are completely unable to reason in a logical, trustable, predictable or explainable manner.  There are situations where it is key to rely on robust symbolic reasoning capabilities in order to: </p> <ul> <li>deduce new facts from known facts  </li> <li>decide in accordance to company policies  </li> </ul> <p>If you need to take business decisions that must be trusted from a risk and compliance perspective, it would be completely unreasonable to  place a statistical bet on the fact that a linguistic parrot can find the right answer.</p> <p>The right approach is to use the right technology for the right use cases. The Athena Owl Agent framework will make your life easy at integrating various approaches to build an hybrid AI solution.</p>"},{"location":"Develop/2-Developers%20roadmap/#choose-the-right-ingredients","title":"Choose the right ingredients","text":"<p>The following table shows various features that are used by three different applications built via the Athena Owl Agent framework.  Depending on the capabilities we want to provide to the employees that will interact with the agent, we will leverage different features. </p> Simple agent with RAG Insurance complaint handling Tax assistant LLM general Q&amp;A extract data, detect intention extract data, detect intention, generate summary of decision outcome RAG queries on company documents queries on company policies Fetch data (tool calling to data APIs) fetch customer and claim data fetch tax payer and vehicle data Decide based on company policies (tool calling to rule service) determine next best action determine eligibility for a tax discount Human in the loop, Open/Closed conversation ask specific questions needed to decide"},{"location":"Develop/2-Developers%20roadmap/#create-a-solution-folder-using-a-template","title":"Create a solution folder using a template","text":"<ul> <li>Install Python 3.11 if needed</li> <li>Run <code>owl-solution-create</code> script</li> </ul>"},{"location":"Develop/2-Developers%20roadmap/#create-a-python-virtual-environment","title":"Create a Python virtual environment","text":"<pre><code>cd $DEMO/ibu_backend/src\npython3 -m venv .venv\nsource .venv/bin/activate\n</code></pre>"},{"location":"Develop/2-Developers%20roadmap/#install-dependencies","title":"Install dependencies","text":"<p>Next, we need to install the Python library dependencies and set the Python path. <pre><code>pip install -r requirements.txt\ncd .. \nsource setpython.sh\n</code></pre></p>"},{"location":"Develop/2-Developers%20roadmap/#declare-the-various-elements-of-the-solution","title":"Declare the various elements of the solution","text":"<p>Edit yaml files (or use admin console)</p>"},{"location":"Develop/2-Developers%20roadmap/#add-hooks-to-custom-code","title":"Add hooks to custom code","text":"<p>This is how we can declare a hook to override the default agent runner class <pre><code>ibu_tax_agent:\n  agent_id: ibu_tax_agent\n  name: Tax Agent\n  description: OpenAI based agent with tool to call tax reduction eligibility service\n  runner_class_name: ibu.llm.agents.IBU_TaxAgent.IBU_TaxAgent                            # custom runner class\n  class_name: athena.llm.agents.base_chain_agent.OwlAgent\n  modelClassName: langchain_openai.ChatOpenAI\n  modelName: gpt-3.5-turbo-0125\n  prompt_ref: tax_first_intent_prompt\n  temperature: 0\n  top_k: 1\n  top_p: 1\n</code></pre></p>"},{"location":"Develop/2-Developers%20roadmap/#setup-unit-and-integration-tests","title":"Setup unit and integration tests","text":"<p>To run all the unit tests: pytest -s tests/ut</p> <p>To run the integration tests: ...</p>"},{"location":"Develop/2-Developers%20roadmap/#implement-custom-code","title":"Implement custom code","text":"<p>We create a file ibu.llm.agents.IBU_TaxAgent.IBU_TaxAgent.py to implement the IBU_TaxAgent class <pre><code>from athena.llm.agents.agent_mgr import OwlAgentDefaultRunner, OwlAgent, get_agent_manager\n\nclass IBU_TaxAgent(OwlAgentDefaultRunner):\n    # your custom code here\n</code></pre></p> <p>Implement an agentic workflow using Langgraph</p>"},{"location":"Develop/2-Developers%20roadmap/#run-tests","title":"Run tests","text":""},{"location":"Develop/2-Developers%20roadmap/#exploratory-testing","title":"Exploratory testing","text":"<p>There are two possible ways to run the out-of-the-box frontend webapp:</p> <ul> <li>Build and run a Docker image for the frontend so that it will be started by <code>docker compose up -d</code> script</li> <li>Run the chatbot webapp locally using node (version &gt;= 18.18) and yarn</li> </ul> <p>If needed, install node:  <pre><code>nvm install --lts\nnvm use --lts\n</code></pre></p> <p>If needed, install yarn: <code>npm install --global yarn</code></p> <pre><code>cd owl-agent-interface\nyarn\nyarn dev\n</code></pre>"},{"location":"Develop/2-Developers%20roadmap/#unit-testing","title":"Unit testing","text":"<p>To run all the unit tests: <pre><code>pytest -s tests/ut\n</code></pre></p>"},{"location":"Develop/2-Developers%20roadmap/#integration-testing","title":"Integration testing","text":"<p>Before running the integration tests, we need to start the Docker containers: <pre><code>cd $DEMO/deployment/local\ndocker compose up -d\n</code></pre></p> <p>We can then run all the integration tests: <pre><code>pytest -s tests/it\n</code></pre></p>"},{"location":"Develop/2-Developers%20roadmap/#package","title":"Package","text":""},{"location":"Develop/2-Developers%20roadmap/#deliver-solution","title":"Deliver solution","text":""},{"location":"Develop/3-Tutorials/","title":"Overview of tutorials","text":""},{"location":"Develop/3-Tutorials/#build-an-owl-app-bootcamp","title":"Build an Owl App Bootcamp","text":"<ul> <li>Bootstrap your app project</li> <li>Add tools to call data and decision APIs</li> <li>Add RAG</li> <li>Add human in the loop controlled by LangGraph</li> <li>Create new backend API endpoints</li> </ul>"},{"location":"Develop/3-Tutorials/#use-various-providers","title":"Use various providers","text":"<ul> <li>Local LLM</li> <li>Mistral</li> <li>OpenAI</li> <li>IBM Granite</li> </ul>"},{"location":"Develop/3-Tutorials/#frontend","title":"Frontend","text":"<ul> <li>Customize the chatbot frontend webapp</li> <li>Integrate the Owl chatbot widget</li> </ul>"},{"location":"Develop/3-Tutorials/Backend/Human%20in%20the%20loop/","title":"Tutorial - Managing a human in the loop","text":""},{"location":"Develop/3-Tutorials/Backend/Human%20in%20the%20loop/#another-heading","title":"Another heading","text":"<p>Some text here</p>"},{"location":"Develop/3-Tutorials/Backend/RAG/","title":"Tutorial - RAG","text":""},{"location":"Develop/3-Tutorials/Backend/RAG/#another-heading","title":"Another heading","text":"<p>Some text here</p>"},{"location":"Develop/3-Tutorials/Backend/Simple%20agent/","title":"Tutorial - Simple Agent with a single LLM","text":""},{"location":"Develop/3-Tutorials/Backend/Simple%20agent/#another-heading","title":"Another heading","text":"<p>Some text here</p>"},{"location":"Develop/3-Tutorials/Backend/Tool%20calling/","title":"Tutorial - Add tool calling","text":""},{"location":"Develop/3-Tutorials/Backend/Tool%20calling/#another-heading","title":"Another heading","text":"<p>Some text here</p>"},{"location":"Develop/4-Reference/Backend%20APIs/","title":"Owl Backend REST APIs","text":"<p>Some text here</p>"},{"location":"Develop/4-Reference/Owl%20core%20library/","title":"Owl Core Python library","text":"<p>Some text here</p>"},{"location":"Get%20started/1-Run%20a%20demo/","title":"Run a demo","text":"<p>We offer you two possible paths.</p>"},{"location":"Get%20started/1-Run%20a%20demo/#run-a-demo-on-your-machine-in-5-minutes","title":"Run a demo on your machine in 5 minutes","text":"<p>Click on the link on the left navigation bar that corresponds to your machine.</p>"},{"location":"Get%20started/1-Run%20a%20demo/#play-with-a-hosted-demo","title":"Play with a hosted demo","text":"<p>Coming soon...</p>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/","title":"Comprehensive Install Guide to run the IBU Insurance demo","text":"<p>This guide provides step-by-step instructions to set up this demo on macOS.</p>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#table-of-contents","title":"Table of Contents","text":"<ul> <li>macOS Installation</li> <li>Step 1: Install Homebrew</li> <li>Step 2: Install Colima</li> <li>Step 3: Install Docker and Docker Compose</li> <li>Step 4: Start Colima</li> <li>Step 5: Clone the Demo Repository</li> <li>Step 6: Setup IBM watsonx.ai</li> <li>Step 7: Setup the Demo environment</li> <li>Step 8: Run the Demo</li> <li>Step 9: Demo scenario</li> </ul>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#macos-installation","title":"macOS Installation","text":""},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-1-install-homebrew","title":"Step 1: Install Homebrew","text":"<p>Homebrew is a package manager for macOS that simplifies the installation of software.</p> <p>To install Homebrew, open your Terminal and run the following command:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>After the installation is complete, verify that Homebrew is installed:</p> <pre><code>brew --version\n</code></pre>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-2-install-colima","title":"Step 2: Install Colima","text":"<p>Colima is a lightweight container runtime for macOS that works with Docker.</p> <p>To install Colima, run:</p> <pre><code>brew install colima\n</code></pre> <p>If you have an Apple Silicon (M1/M2) Mac, you need to install Rosetta:</p> <pre><code>softwareupdate --install-rosetta\n</code></pre>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-3-install-docker-and-docker-compose","title":"Step 3: Install Docker and Docker Compose","text":"<p>Docker is essential for containerization, and Docker Compose helps in managing multi-container applications.</p> <p>To install Docker and Docker Compose, run:</p> <pre><code>brew install docker docker-compose\n</code></pre>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-4-start-colima","title":"Step 4: Start Colima","text":"<p>Now, you can start Colima with the desired configuration. If you have an Intel Mac, run:</p> <pre><code>colima start --cpu 4 --memory 8\n</code></pre> <p>For Apple Silicon (M1/M2) Macs, run:</p> <pre><code>colima start --cpu 4 --memory 8 --vm-type=vz --vz-rosetta\n</code></pre>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-5-clone-the-demo-repository","title":"Step 5: Clone the Demo Repository","text":"<p>Now that your environment is set up, you can clone the <code>athena-owl-demos</code> repository. If you don't have Git installed, you can do so by running:</p> <pre><code>brew install git\n</code></pre> <p>Then, clone the repository and navigate into the demo directory:</p> <pre><code>git clone https://github.com/AthenaDecisionSystems/athena-owl-demos.git\ncd athena-owl-demos\n</code></pre>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-6-setup-ibm-watsonxai","title":"Step 6: Setup IBM watsonx.ai","text":"<ol> <li> <p>Create an IBM watsonx.ai account    Visit the IBM watsonx.ai page and follow the links to set up a Cloud instance.</p> </li> <li> <p>Generate an API key    In your IBM Cloud account, go to <code>Profile and settings</code> and generate an API key.</p> </li> <li> <p>Instantiate a watsonx.ai service    From your IBM Cloud account, create a new watsonx.ai service instance.</p> </li> <li> <p>Copy your watsonx.ai configuration parameters </p> </li> </ol> <pre><code>WATSONX_URL=&lt;your watsonx.ai URL&gt; # example value = https://us-south.ml.cloud.ibm.com/\nWATSONX_APIKEY=&lt;your watsonx.ai API Key&gt;\nWATSONX_PROJECT_ID=&lt;your watsonx.ai Project ID&gt;\n</code></pre>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-7-setup-the-demo-environment","title":"Step 7: Setup the demo environment","text":"<p>Navigate to the IBU Insurance demo directory:</p> <pre><code>cd IBU-insurance-demo\n</code></pre> <p>Create your <code>.env</code> file. It contains some configuration parameters. Paste your watsonx.ai configuration parameters:</p> <pre><code>WATSONX_URL=&lt;your watsonx.ai URL&gt; # example value = https://us-south.ml.cloud.ibm.com/\nWATSONX_APIKEY=&lt;your watsonx.ai API Key&gt;\nWATSONX_PROJECT_ID=&lt;your watsonx.ai Project ID&gt;\n</code></pre> <p>If you plan to use other LLMs for which a key is needed, paste them here:</p> <pre><code># Define the IBM KEY to access models deployed on watsonx.ai\nIBM_API_KEY=USE_YOUR_IBM_API_KEY\n\n# Only when you want to use one of Open AI model\nOPENAI_API_KEY=USE_YOUR_OPENAI_API_KEY\n\n# Only when you want to use one a Mistral AI model\nMISTRAL_API_KEY=USE_YOUR_MISTRAL_API_KEY\n\n# Use Tavily to do search on last news, that could be interesting to validate tool calling\nTAVILY_API_KEY=USE_YOUR_TAVILY_API_KEY\n\n# if you want to get traces in Langchain tracing - this is optional\nLANGCHAIN_TRACING_V2=false\nLANGCHAIN_API_KEY=USE_YOUR_LANGCHAIN_KEY\n\n# If you want to use one of the Anthropic Claude models\nANTHROPIC_API_KEY=USE_YOUR_ANTHROPIC_KEY\n</code></pre>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-8-run-the-demo","title":"Step 8: Run the Demo","text":"<p>Once everything is set up, you can run the project by executing:</p> <pre><code>cd deployment/local\ndocker-compose up -d\n</code></pre> <p>After the project is running, you can access the Owl Agent at:</p> <pre><code>http://localhost:3000\n</code></pre>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-9-demo-scenario","title":"Step 9: Demo Scenario","text":"<p>Your environment should now be fully set up and ready for development and testing. If you encounter any issues, please refer to the documentation or raise an issue in the GitHub repository.</p>"},{"location":"Get%20started/3-Install%20and%20run%20on%20Windows/","title":"Install and run on Windows","text":"<p>Coming soon...</p>"},{"location":"Support/","title":"Getting support","text":""},{"location":"Support/#another-heading","title":"Another heading","text":"<p>Some text here</p>"},{"location":"Technology/Deployment%20options/","title":"Athena Hybrid AI Technology","text":""},{"location":"Technology/Deployment%20options/#another-heading","title":"Another heading","text":"<p>Some text here</p>"},{"location":"Technology/Enterprise%20AI/","title":"Architecture for Enterprise AI","text":""},{"location":"Technology/Enterprise%20AI/#another-heading","title":"Another heading","text":"<p>Some text here</p>"},{"location":"Technology/Hybrid%20AI/","title":"Athena Hybrid AI Technology","text":""},{"location":"Technology/Hybrid%20AI/#another-heading","title":"Another heading","text":"<p>Some text here</p>"}]}